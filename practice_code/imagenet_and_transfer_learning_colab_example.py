# -*- coding: utf-8 -*-
"""imagenet_and_transfer_learning_colab_example.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13A3NScUwJMCjBjkwq7VrdjLeXz2PMgLI

ImageNet example

* copy the directory available at https://drive.google.com/drive/u/1/folders/1-OrigNje-etq-pFyS-XenhiuhfJ7aEO2
 
> * put it in drive/My Drive/public/. (make sure you have a directory 'drive/My Drive/public/imagenet_install')


* Your new images should be put in 'drive/My Drive/public/data/images/.'
"""

from google.colab import drive
drive.mount('/content/drive')

"""configuration to download a pretrained model"""

!pip install pretrainedmodels
!python ./drive/'My Drive'/public/imagenet_install/setup.py install

"""download a pretrained model and prepare a transformation function"""

import pretrainedmodels
import pretrainedmodels.utils as utils
import torch
import os
import warnings
warnings.filterwarnings("ignore")

# load the model if you have downloaded and saved a model. 
model_name = 'nasnetalarge' # could be fbresnet152 or inceptionresnetv2
model_file = 'drive/My Drive/public/results/imagenet_nasnetalarge.pth'
if os.path.exists(model_file):
  model = torch.load(model_file)
else:
  model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet')
  # save the model for later
  if not os.path.exists('drive/My Drive/public/results'):
      os.mkdir('drive/My Drive/public/results')    
  torch.save(model, model_file)

model.eval()
import pretrainedmodels.utils as utils
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

load_img = utils.LoadImage()
transform_img = utils.TransformImage(model) #input_size = (331, 331)

"""imagenet class names"""

import csv
name_file = 'drive/My Drive/public/imagenet_install/class_names.csv'

imagenet_class  = {}
file_in = csv.reader(open(name_file))
for row in file_in:
  imagenet_class[int(row[0])] = row[1]

print(imagenet_class)

"""*classification* of a single image"""

# your file name
img_file = './drive/My Drive/public/data/images/cat_224.jpg'

input_img = load_img(img_file)
input_data = transform_img(input_img).to(device) # 3x400x225 -> 3x331x331 size may differ
input_data = input_data.unsqueeze(0)           # 3x331x331 -> 1x3x331x331

output_logits = model(input_data) # 1x1000

print("{} is [{}: {}]".format(img_file ,output_logits.argmax(),
                           imagenet_class[int(output_logits.argmax())]))

"""classification of all the files in a directory
./drive/My Drive/public/data/images/
"""

import glob
dir_path = './drive/My Drive/public/data/images/'
img_list = glob.glob(dir_path+'*.*')

for img_file in img_list:
  input_img = load_img(img_file)
  input_data = transform_img(input_img).to(device) # -> 3x331x331
  input_data = input_data.unsqueeze(0)    # 3x331x331 -> 1x3x331x331
  
  output_logits = model(input_data) # 1x1000

  print("{} is [{}: {}]".format(img_file.split('/')[-1] ,output_logits.argmax(),
                           imagenet_class[int(output_logits.argmax())]))

"""# **Transfer** **Learning**

Let's see ours pre-trained model
"""

#print(model) # pretrained NASNetALarge which trained by imagenet data
model.last_linear.in_features

"""**We want to reuse pre-trained model's parameters except the 'last_linear' layer**

Let's redefine the model
"""

from pretrainedmodels.models.nasnet import NASNetALarge
output_dim = 5

class TransferedModel(NASNetALarge) :
  def __init__(self):
    super(TransferedModel, self).__init__()
    
    self.last_linear = nn.Linear(model.last_linear.in_features, output_dim)

"""we train only the 'last_linear' layer"""

import torch.nn as nn

pretrained_dict = model.state_dict()

model = TransferedModel().to(device)
model_dict = model.state_dict()

## load pretrained model's parameter
# 1. filter out unnecessary keys
pretrained_dict = {k: v for k, v in pretrained_dict.items() if not ('last_linear' in k) }
# 2. overwrite entries in the existing state dict
model_dict.update(pretrained_dict)
# 3. load the new state dict
model.load_state_dict(model_dict)

learning_rate = 0.0005
loss_func = nn.CrossEntropyLoss()

## optimizer without pretrained parameters
for name, param in model.named_parameters() :
  if 'last_linear' in name :
    param.requires_grad = True
  else :
    param.requires_grad = False
params = filter(lambda p: p.requires_grad, model.parameters())
optimizer = torch.optim.Adam(params, lr=learning_rate)

"""*   data loader"""

from torchvision import datasets

batch_size = 64 # try different batch_size values
data_dir = './drive/My Drive/public/'
train_dir = 'train'
valid_dir = 'valid1'

train_set = datasets.ImageFolder(data_dir+train_dir, transform_img)
valid_set = datasets.ImageFolder(data_dir+valid_dir, transform_img)


train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,
                                              shuffle=True, num_workers=4)          
valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size,
                                              shuffle=True, num_workers=4)

train_size = len(train_set)
valid_size = len(valid_set)

class_names = train_set.classes

print(class_names) 
print(f'Train image size: {train_size}')
print(f'Validation image size: {valid_size}')

"""*   training the new network with new dataset"""

import os

result_dir = 'drive/My Drive/public/results/'
num_epoch = 3 # try with different epochs and find the best epoch

if not os.path.exists(result_dir):
    os.mkdir(result_dir)    
    
for i in range(num_epoch):
    model.train()
    for j, [image,label] in enumerate(train_loader):
        x = image.to(device)
        y_= label.to(device)
        
        optimizer.zero_grad()
        output = model(x)
        loss = loss_func(output,y_)
        loss.backward()
        optimizer.step()
        
        if j % 30 == 0:
            print(i,j, loss.data.cpu())
    
    model.eval()
    hits = 0
    for k, [image,label] in enumerate(valid_loader):
        x = image.to(device)
        y_= label.to(device)

        output = model(x)
        y_est = output.argmax(1)
        hits = hits + sum(y_est == y_).cpu()
    print('Hits', int(hits), 'Accuracy', float(hits/(valid_size+0.0)))    

torch.save(model, result_dir + 'teamX.model')
print('training is done by max_epochs', num_epoch)

"""*   test your model"""

test_batch_size = 10
result_dir = 'drive/My Drive/public/results/'
model_name = 'teamX.model'

model = torch.load(result_dir + model_name)
model.to(device)
model.eval()

test_dir = './drive/My Drive/public/valid2'
test_set = datasets.ImageFolder(test_dir, transform_img) #test_transform)
              
test_loader = torch.utils.data.DataLoader(test_set, batch_size=test_batch_size,
                                          shuffle=False, num_workers=4)

hits = 0
for k,[image,label] in enumerate(test_loader):
    x = image.to(device)
    y_= label.to(device)
  
    output = model(x)
    y_est = output.argmax(1)
    print('Target', label.numpy(), 'Prediction', y_est.cpu().numpy())
    hits = hits + sum(y_est == y_)
print('hits', int(hits),'accuracy', float(hits/(len(test_set)+0.0)))

"""*   classify one image"""

from skimage import io
from torchvision import transforms

img_name = './drive/My Drive/public/data/test/test1.jpg'
test_img = io.imread(img_name)
test_img = transforms.ToPILImage()(test_img)
test_data = transform_img(test_img).unsqueeze(0).to(device)

output=model(test_data)

class_id = output.argmax(dim=1).cpu().numpy()[0]
print(img_name.split('/')[-1], '==>', class_id, class_names[class_id])

"""*   classify all images in a directory"""

from skimage import io
import glob

img_dir = 'drive/My Drive/public/data/test/'
file_list = glob.glob(img_dir + '*.*')
for img_name in file_list:
  test_img = io.imread(img_name)
  test_img = transforms.ToPILImage()(test_img)
  test_data = transform_img(test_img).unsqueeze(0).to(device)
  output=model(test_data)

  class_id = output.argmax(dim=1).cpu().numpy()[0]
  print(img_name.split('/')[-1], '==>', class_id, class_names[class_id])

"""the end!
----
"""