{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"my_cnn_toy_dog_cat.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"M6JApzL6g_jQ","colab_type":"text"},"source":["**sample code for hackathon with your data in directory** (by hchoi@handong.edu) \n","\n","put class directories in ./train/, ./val/, and ./test/.\n","\n","for example, with 'dog', 'cat' classes\n","* ./train/dog/*.jpg\n","* ./train/cat/*.jpg\n","* ./val/dog/*.jpg\n","* ./val/cat/*.jpg\n","* ./test/dog/*.jpg\n","* ./test/cat/*.jpg\n","\n","**submit your teamX.model and teamX.transform files. **"]},{"cell_type":"code","metadata":{"id":"UUQQHb4HKf8_","colab_type":"code","outputId":"ccc3c93f-4dcc-4930-fa09-be32104adfe6","executionInfo":{"status":"ok","timestamp":1579101584561,"user_tz":-540,"elapsed":9048,"user":{"displayName":"엄하영","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZ4gUuvfGnvca_BFi9SSTRwJejg4x6ea63YLaf=s64","userId":"05255339759085456823"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NM4cBDItKoxt","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.init as init\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","import os\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UQt5CUzzh_z0","colab_type":"text"},"source":["**try to change the network architecture**"]},{"cell_type":"code","metadata":{"id":"NG2Rk8Y_Kvk-","colab_type":"code","colab":{}},"source":["class MyCNN(nn.Module):\n","    def __init__(self, input_size=32, output_dim=10):\n","        super(MyCNN,self).__init__()\n","\n","        self.input_size=input_size # 100 x 100\n","        self.output_dim=output_dim\n","\n","        self.layer = nn.Sequential(\n","            nn.Conv2d(3,64,3,padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2), # 64 x (50 x 50)\n","\n","            nn.Conv2d(64,128,3,padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2), # 128 x (25 x 25)\n","            \n","            nn.Conv2d(128,256,3,padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2), # 256 x (12 x 12)\n","\n","            nn.Conv2d(256,128,3,padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2), # 128 x (6 x 6) \n","            \n","            nn.Conv2d(128,64,3,padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64,32,3,padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2) # 32 x (3 x 3) \n","        )\n","        self.fc_layer = nn.Sequential(\n","            nn.Linear(32*3*3,100),\n","            nn.ReLU(),\n","            nn.Linear(100,output_dim)\n","        )       \n","        \n","    def forward(self,x):\n","        batch_size, c, h, w = x.data.size()\n","        out = self.layer(x)\n","        out = out.view(batch_size,-1)\n","        out = self.fc_layer(out)\n","\n","        return out\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wtuFoEzamuB1","colab_type":"text"},"source":["**try to change learning rate and input_size**\n","\n","* be carefule with the input size, you have to change the network architecture accordingly"]},{"cell_type":"code","metadata":{"id":"mc2woMezKOVN","colab_type":"code","colab":{}},"source":["learning_rate = 0.0005\n","input_size=(100, 100)\n","output_dim=2\n","\n","model = MyCNN(input_size=input_size, output_dim=output_dim).cuda()\n","loss_func = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PthV1tdXiE1S","colab_type":"text"},"source":["**try to change transform function**"]},{"cell_type":"code","metadata":{"id":"RtKn4Z--LLt8","colab_type":"code","colab":{}},"source":["data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomRotation(5),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomResizedCrop(input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.5, 0.5, 0.5], [0.3, 0.3, 0.3])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.5, 0.5, 0.5], [0.3, 0.3, 0.3])\n","    ]),\n","}\n","test_transform = data_transforms['val']\n","torch.save(test_transform, 'drive/My Drive/public/results/teamX.transform')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rC_j9RVnnEyN","colab_type":"text"},"source":["**data loader**"]},{"cell_type":"code","metadata":{"id":"Xuya0_VVQzZv","colab_type":"code","outputId":"867f6f73-0921-45f6-eedd-75f0c23c0846","executionInfo":{"status":"error","timestamp":1579101645083,"user_tz":-540,"elapsed":1155,"user":{"displayName":"엄하영","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDZ4gUuvfGnvca_BFi9SSTRwJejg4x6ea63YLaf=s64","userId":"05255339759085456823"}},"colab":{"base_uri":"https://localhost:8080/","height":394}},"source":["batch_size = 64\n","data_dir = './drive/My Drive/public/data/toy_dog_cat'\n","\n","image_datasets = {x: datasets.ImageFolder('./drive/My Drive/public/data/toy_dog_cat/'+x,\n","                                          data_transforms[x])\n","                  for x in ['train', 'val']}\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n","                                              shuffle=True, num_workers=4)\n","                  for x in ['train', 'val']}\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","class_names = image_datasets['train'].classes\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(class_names) \n","print(f'Train image size: {dataset_sizes[\"train\"]}')\n","print(f'Validation image size: {dataset_sizes[\"val\"]}')"],"execution_count":0,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-e5d9acf267b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m image_datasets = {x: datasets.ImageFolder('./drive/My Drive/public/data/toy_dog_cat/'+x,\n\u001b[1;32m      5\u001b[0m                                           data_transforms[x])\n\u001b[0;32m----> 6\u001b[0;31m                   for x in ['train', 'val']}\n\u001b[0m\u001b[1;32m      7\u001b[0m dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n\u001b[1;32m      8\u001b[0m                                               shuffle=True, num_workers=4)\n","\u001b[0;32m<ipython-input-6-e5d9acf267b3>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m image_datasets = {x: datasets.ImageFolder('./drive/My Drive/public/data/toy_dog_cat/'+x,\n\u001b[1;32m      5\u001b[0m                                           data_transforms[x])\n\u001b[0;32m----> 6\u001b[0;31m                   for x in ['train', 'val']}\n\u001b[0m\u001b[1;32m      7\u001b[0m dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n\u001b[1;32m      8\u001b[0m                                               shuffle=True, num_workers=4)\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     91\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m     92\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# Faster and available in Python 3.5 and above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './drive/My Drive/public/data/toy_dog_cat/train'"]}]},{"cell_type":"markdown","metadata":{"id":"lBM8GgSNiZ6m","colab_type":"text"},"source":["**training**"]},{"cell_type":"code","metadata":{"id":"9eDi5Aeba8Ha","colab_type":"code","colab":{}},"source":["num_epoch = 10\n","\n","if not os.path.exists('drive/My Drive/public/results'):\n","    os.mkdir('drive/My Drive/public/results')    \n","    \n","for i in range(num_epoch):\n","    for j, [image,label] in enumerate(dataloaders['train']):\n","        x = image.cuda()\n","        y_= label.cuda()\n","        \n","        optimizer.zero_grad()\n","        output = model(x)\n","        loss = loss_func(output,y_)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if j % 100 == 0:\n","            print(loss.data,i,j)\n","    \n","    with torch.no_grad(): \n","      hits = 0\n","      for k,[image,label] in enumerate(dataloaders['val']):\n","          x = image.cuda()\n","          y_= label.cuda()\n","        \n","          output = model(x)\n","          y_est = output.argmax(1)\n","          hits = hits + sum(y_est == y_)\n","      print('hits, accuracy', hits, hits/(dataset_sizes[\"val\"]+0.0))    \n","    torch.save(model,'drive/My Drive/public/results/teamX.model')\n","            \n","#param_list = list(model.children())\n","#print(param_list)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"klAJEc3Pg9-d","colab_type":"text"},"source":["**TA will check your results based on teamX.model and teamX.transform as below**"]},{"cell_type":"code","metadata":{"id":"9rCqCX6NjQZ1","colab_type":"code","colab":{}},"source":["test_batch_size = 4\n","model = torch.load('drive/My Drive/public/results/teamX.model')\n","test_transform = torch.load('drive/My Drive/public/results/teamX.transform')\n","model.cuda()\n","model.eval()\n","  \n","image_testset = datasets.ImageFolder('./drive/My Drive/public/data/toy_dog_cat/test',\n","                                          test_transform)\n","              \n","testloader = torch.utils.data.DataLoader(image_testset, batch_size=test_batch_size,\n","                                          shuffle=False, num_workers=4)\n","\n","hits = 0\n","for k,[image,label] in enumerate(testloader):\n","    x = image.cuda()\n","    y_= label.cuda()\n","  \n","    output = model(x)\n","    print(output.argmax(1), label)\n","    y_est = output.argmax(1)\n","    hits = hits + sum(y_est == y_)\n","print('hits, accuracy', hits, hits/(len(image_testset)+0.0))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r6kZDqJpi74n","colab_type":"text"},"source":["The end! (hchoi@handong.edu)"]}]}